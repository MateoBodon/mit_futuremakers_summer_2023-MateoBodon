# Classification Loss Functions

## Topics covered in today's module
* Kullback Leibler Divergence Loss
* Binary Cross-Entropy
* Categorical Cross-Entropy
* Sparse Categorical Cross-Entropy

## Main takeaways from doing today's assignment
* Explored different types of classification loss functions and their importance in training machine learning models.
* Learned and implemented Kullback-Leibler Divergence Loss, Binary Cross-Entropy, Categorical Cross-Entropy, and Sparse Categorical Cross-Entropy in Python.
* Understood the mathematical foundations and practical applications of these loss functions.

## Challenging, interesting, or exciting aspects of today's assignment
* The mathematical derivations of the loss functions were challenging but rewarding to understand.
* It was interesting to see the impact of different loss functions on the training of a model.
* Experimenting with different loss functions on a dataset and observing the changes in model performance was exciting.
* Understanding the subtleties between similar loss functions like Binary and Categorical Cross-Entropy, as well as Sparse and Categorical Cross-Entropy, was intriguing.

## Additional resources used 
* I've started to reference the textbook "An Introduction to Statistical Learning" - https://www.statlearning.com/
* Classification Loss Function - https://en.wikipedia.org/wiki/Loss_functions_for_classification
* Loss Functons - https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html
* I also used ChatGPT frequently for this module to help explain terms that were less familiar to me.
* I have a Folder of notes on Python that I refer to a lot. It includes pretty much all the Python Fundamentals. The notes are a compilation of course-provided notes and ones of my own.
* After solving problems, I commonly ask ChatGPT about my solution to see how it can be improved or done differently. However, the answers provided in the notebook represent my original attempt.
