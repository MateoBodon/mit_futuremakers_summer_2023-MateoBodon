# Regression Loss Functions

## Topics covered in today's module
* Mean Squared Error
* Mean Absolute Error
* Mean Squared Logarithm Error

## Main takeaways from doing today's assignment
* Learned about the differences between Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Squared Logarithmic Error (MSLE) as loss functions in regression problems.
* Gained hands-on experience in implementing these loss functions using NumPy and calculating them from scratch.
* Understood the circumstances under which each loss function is preferred over the others. MSE is sensitive to outliers, MAE is less sensitive, and MSLE is used when exponential growth is expected.

## Challenging, interesting, or exciting aspects of today's assignment
* Found it challenging to initially grasp the difference between the loss functions and how each one has a different sensitivity to outliers.
* Interesting to delve into the mathematical reasoning behind each loss function and see it in action with synthetic data.
* Implementing these functions from scratch was challenging but helped in solidifying understanding.

## Additional resources used 
* I've started to reference the textbook "An Introduction to Statistical Learning" - https://www.statlearning.com/
* Loss Functions - https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23
* Loss Functions (2) - https://en.wikipedia.org/wiki/Loss_function
* I also used ChatGPT frequently for this module to help explain terms that were less familiar to me.
* I have a Folder of notes on Python that I refer to a lot. It includes pretty much all the Python Fundamentals. The notes are a compilation of course-provided notes and ones of my own.
* After solving problems, I commonly ask ChatGPT about my solution to see how it can be improved or done differently. However, the answers provided in the notebook represent my original attempt.
