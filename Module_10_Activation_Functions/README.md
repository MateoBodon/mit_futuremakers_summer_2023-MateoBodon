# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment
* Gained a comprehensive understanding of different activation functions including Sigmoid, Tanh, ReLU, Swish, GeLU, and SeLU.
* Learned about the impact of activation functions on neural networks, particularly in terms of performance and susceptibility to issues like vanishing and exploding gradients.
* Explored the concept of 'dying ReLU' problem and how it affects the learning process of neural networks.
* Visualized the effect of ReLU on feature maps, gaining insights into how it introduces non-linearity and helps in learning complex patterns.
* Acquired a deeper understanding of vanishing and exploding gradients, and how they can detrimentally affect the training of deep neural networks.

## Challenging, interesting, or exciting aspects of today's assignment
* Visualizing the impact of ReLU on feature maps was an interesting aspect, as it provided a tangible way to understand the role of activation functions in neural networks.
* The concept of 'dying ReLU' was challenging to understand at first, but realizing its implications was a significant learning moment.
* Exploring the advanced activation functions (Swish, GeLU, SeLU) was exciting as it opened up new possibilities for optimizing neural network performance.

## Additional resources used 
* I've started to reference the textbook "An Introduction to Statistical Learning" - https://www.statlearning.com/
* Vanishing Gradients - https://stats.stackexchange.com/questions/301285/what-is-vanishing-gradiente
* Visualizing Activation Functions - https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0
* Activation Functions - https://www.v7labs.com/blog/neural-networks-activation-functions
* Activation Functions (2) - https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
* I also used ChatGPT frequently for this module to help explain terms that were less familiar to me.
* I have a Folder of notes on Python that I refer to a lot. It includes pretty much all the Python Fundamentals. The notes are a compilation of course-provided notes and ones of my own.
* After solving problems, I commonly ask ChatGPT about my solution to see how it can be improved or done differently. However, the answers provided in the notebook represent my original attempt.
